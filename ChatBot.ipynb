{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import docx\n",
    "import textract\n",
    "from pptx import Presentation\n",
    "import docx\n",
    "\n",
    "import os\n",
    "#from collections import defaultdict\n",
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import sys, getopt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "#from operator import add\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n",
    "#nltk.download('maxent_ne_chunker') # first-time use only\n",
    "#nltk.download('words')# first-time use only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesInAllSubFolder(folder):\n",
    "    all_file_list = []\n",
    "    with os.scandir(folder) as entries:\n",
    "        for entry in entries:\n",
    "            filename = os.path.join(folder, entry.name)\n",
    "            abs_fname = os.path.abspath(filename)\n",
    "            if os.path.isfile(abs_fname):\n",
    "                #print(filename +\" is a file\")\n",
    "                all_file_list.append(abs_fname)\n",
    "            else:\n",
    "                #print(filename +\" is a folder\")\n",
    "                all_file_list = all_file_list + getFilesInAllSubFolder(filename)\n",
    "    \n",
    "    return all_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = re.sub('ï¿½', '', text)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#text = \"jeevitha\\n how \\nare you\"\n",
    "#LemNormalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used document similarity to generate response\n",
    "\n",
    "def response(user_response, resultDict, sent_tokens):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    #print(\"sent_tokens_all_files\", sent_tokens)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    #print(idx)\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    sent_tokens.remove(user_response)\n",
    "    #print(\"final dict\", final_dict)\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Want to try something else?\"\n",
    "        return robo_response, None\n",
    "    else:\n",
    "        response_sen1 = sent_tokens[idx]\n",
    "        robo_response = robo_response+sent_tokens[idx]+sent_tokens[idx+1]+sent_tokens[idx+2]+sent_tokens[idx+3]\n",
    "        if response_sen1 in resultDict:\n",
    "            fvalue = resultDict[response_sen1]\n",
    "        else:\n",
    "            fvalue = \"could not find\"\n",
    "        print(\"response_sen1--\", response_sen1)\n",
    "        print(\"final dict\", fvalue)\n",
    "        json_dict = {\"ans\" : robo_response, \"filePath\" : fvalue[0], \"pageNo\" : fvalue[1]}\n",
    "        return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_list = defaultdict(list)\n",
    "\n",
    "set_sent_tokens = set()\n",
    "def sen_dict(clean_txt, final_dict, fname, p_num):\n",
    "    sent_tokens = nltk.sent_tokenize(clean_txt.lower())# converts to list of sentences\n",
    "    #print(\"sent_tokens inside sen_dict\",sent_tokens)\n",
    "    for i in sent_tokens:\n",
    "        if i in set_sent_tokens:\n",
    "            final_dict[i].append([fname,p_num])\n",
    "            final_dict.update(final_dict)\n",
    "            #print(\"final_list1\",final_dict[i])\n",
    "            #print(\"final_list1\",final_dict)\n",
    "        else:\n",
    "            dict1 = {i : [fname,p_num]}\n",
    "            final_dict.update(dict1)\n",
    "            #print(\"final_dict\",final_dict)\n",
    "            set_sent_tokens.add(i)\n",
    "        #print(\"set_sent_tokens\",set_sent_tokens)\n",
    "    return final_dict, sent_tokens    \n",
    "    #print(final_dict)\n",
    "    #print(\"----------------------------------------\")\n",
    "\n",
    "#clean_txt = \"cloud. jee. harsha. cloud. dar. cloud.\"   \n",
    "#final_dict, sent_tokens = sen_dict(clean_txt, {}, \"fname.pdf\", 10)\n",
    "#clean_txt = \"cloud.\"\n",
    "#print(\"results here\",final_dict)\n",
    "#final_dict, sent_tokens = sen_dict(clean_txt, final_dict, \"chatbot.pptx\", 5)\n",
    "#clean_txt = \"dar. cloud.\"\n",
    "#sen_dict(clean_txt, {}, \"chatbot.pptx\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, final_dict):\n",
    "    output = io.StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    page_no = 0\n",
    "    sent_tokens_all = []\n",
    "    infile = open(fname, 'rb')\n",
    "    finalText=\"\"\n",
    "    #for page in PDFPage.get_pages(infile, pagenums):\n",
    "    for pageNumber, page in enumerate(PDFPage.get_pages(infile)):\n",
    "        if pageNumber == page_no:\n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "        converter.close()\n",
    "        text = output.getvalue()\n",
    "        clean_txt = clean_text(text)\n",
    "        result, sent_tokens = sen_dict(clean_txt,final_dict,fname,page_no+1)\n",
    "        finalText = finalText+text\n",
    "        sent_tokens_all = sent_tokens_all + sent_tokens\n",
    "        #print(page_no+1)\n",
    "        #print(result)\n",
    "        output.truncate(0)\n",
    "        output.seek(0)\n",
    "        #print(finalText)\n",
    "        page_no += 1\n",
    "    #print(\"sent_tokens_all\", sent_tokens_all)    \n",
    "    return finalText, result, sent_tokens_all\n",
    "    infile.close()\n",
    "\n",
    "#final_dict = {}\n",
    "#convert('docs/pdfs/forumQAchatbot.pdf', final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to train a chatbot\n",
      "response_sen1-- for training our chatbot we also need data; the conversation between a user and a chatbot is the data on which we need to train our models.\n",
      "final dict ['/Users/jeeva/AlniTek/projects/knowledgebase-bot/docs/pdfs/Building_Chatbots_with_Python.pdf', 146]\n",
      "result here {'ans': 'for training our chatbot we also need data; the conversation between a user and a chatbot is the data on which we need to train our models.sometimes it becomes difficult to find a dataset freely on the web that fits our own need.we should spend the time we need to get the data gathered.we can ask our friends and family to provide us the sample conversation text of how they would interact with a kind of bot you are building.', 'filePath': '/Users/jeeva/AlniTek/projects/knowledgebase-bot/docs/pdfs/Building_Chatbots_with_Python.pdf', 'pageNo': 146}\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'docs'\n",
    "allFileList = getFilesInAllSubFolder(corpus_folder)\n",
    "\n",
    "final_dict = {}\n",
    "pdf_sent_tokens_all = []\n",
    "ppt_sent_tokens_all = []\n",
    "append_write = 'w'\n",
    "text_file = open('sample1.txt', append_write)\n",
    "for filename in allFileList:\n",
    "    #print(filename)\n",
    "    \n",
    "    if filename.endswith(\".pdf\"):\n",
    "        try:\n",
    "            with open('sample1.txt', append_write) as text_file:\n",
    "                finalText, resultDict, sent_tokens_all = convert(filename, final_dict) #get string of text content of pdf\n",
    "                #print(\"inside pdf all\",sent_tokens_all)\n",
    "                pdf_sent_tokens_all = pdf_sent_tokens_all + sent_tokens_all\n",
    "                text_file.write(finalText)\n",
    "                #print(\"inside pdf\", resultDict)\n",
    "            text_file.close()\n",
    "            pdf_file.close()\n",
    "        except:\n",
    "            #print(\"skipping file due to encoding error \"+filename)\n",
    "            file = open(filename, encoding=\"iso-8859-1\")\n",
    "            text = file.read()\n",
    "    elif filename.endswith(\".doc\") or filename.endswith(\".docx\"):\n",
    "        with open('sample1.txt', append_write) as text_file:\n",
    "            textDoc = textract.process(filename)\n",
    "            text_file.write(str(textDoc.decode(\"utf-8\")))\n",
    "            text_file.close()\n",
    "            #doc = docx.Document(filename)\n",
    "            # read in each paragraph in file\n",
    "            #result = [p.text for p in doc.paragraphs]\n",
    "            #print(\"inside doc\")\n",
    "            #print(textDoc)\n",
    "    elif filename.endswith(\".pptx\"):\n",
    "        #print(filename)\n",
    "        page_no = 1\n",
    "        with open(filename,'rb') as pptx_file, open('sample1.txt', append_write) as text_file:\n",
    "            prs = Presentation(pptx_file)\n",
    "            for slide in prs.slides:\n",
    "                clean_txt = \"\"\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        text_file.write(shape.text)\n",
    "                        sent_shape = shape.text\n",
    "                        if((shape.text).endswith(\".\")):\n",
    "                            text_file.write(\"\")\n",
    "                            sent_shape += \"\"\n",
    "                        else:\n",
    "                            text_file.write(\". \")\n",
    "                            sent_shape += \". \"\n",
    "                        #print(shape.text)\n",
    "                        clean_txt = clean_txt +\" \"+ clean_text(shape.text)\n",
    "                #print(\"cleaning\",clean_txt)\n",
    "                resultDict, sent_tokens = sen_dict(clean_txt,final_dict,filename,page_no)\n",
    "                page_no += 1\n",
    "                ppt_sent_tokens_all = ppt_sent_tokens_all + sent_tokens\n",
    "            #print(\"sent_tokens_all_ppts\", ppt_sent_tokens_all)\n",
    "            #print(resultDict)\n",
    "            #print(\"______________________________\")\n",
    "            text_file.close()\n",
    "        pptx_file.close()\n",
    "    #elif filename.endswith(\".xls\") or filename.endswith(\".xlsx\"):\n",
    "        #with open(filename,'rb') as excel_file, open('sample1.txt', append_write) as text_file:\n",
    "            #print(text)\n",
    "            #print(\"inside ppt\")\n",
    "    if text_file.mode == 'w':\n",
    "        append_write = 'a'\n",
    "        resultDict = final_dict\n",
    "text_file.close()\n",
    "\n",
    "#a = 'after this, the bot goes on learning from interaction and then it follows that flow of communication, which it had in the past with another user.'\n",
    "\n",
    "#if a in resultDict:\n",
    "    #print(\"resulting dict is-\", resultDict[a])\n",
    "\n",
    "res_sen_list = pdf_sent_tokens_all+ppt_sent_tokens_all\n",
    "#print(\"____________res_sen_list__________________\")\n",
    "#print(ppt_sent_tokens_all)\n",
    "#print(resultDict)\n",
    "\n",
    "\n",
    "flag=True\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    result = response(user_response, resultDict, res_sen_list)\n",
    "    print(\"result here\",result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#f=open('sample1.txt','r',errors = 'ignore')\n",
    "#raw=f.read()\n",
    "#raw=raw.lower()# converts to lowercase\n",
    "\n",
    "#sent_tokens = nltk.sent_tokenize(clean_text(raw))# converts to list of sentences\n",
    "\n",
    "#print(\"sent_tokens_all_files\", sent_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
